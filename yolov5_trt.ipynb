{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "An example that uses TensorRT's Python api to make inferences.\n",
    "\"\"\"\n",
    "\n",
    "import ctypes\n",
    "import os\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_W = 640\n",
    "INPUT_H = 640\n",
    "CONF_THRESH = 0.4\n",
    "IOU_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    \"\"\"\n",
    "    description: Plots one bounding box on image img,\n",
    "                 this function comes from YoLov5 project.\n",
    "    param:\n",
    "        x:      a box likes [x1,y1,x2,y2]\n",
    "        img:    a opencv image object\n",
    "        color:  color to draw rectangle, such as (0,255,0)\n",
    "        label:  str\n",
    "        line_thickness: int\n",
    "    return:\n",
    "        no return\n",
    "\n",
    "    \"\"\"\n",
    "    tl = (\n",
    "            line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1\n",
    "    )  # line/font thickness\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (c1[0], c1[1] - 2),\n",
    "            0,\n",
    "            tl / 3,\n",
    "            [225, 255, 255],\n",
    "            thickness=tf,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class YoLov5TRT(object):\n",
    "    \"\"\"\n",
    "    description: A YOLOv5 class that warps TensorRT ops, preprocess and postprocess ops.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, engine_file_path):\n",
    "        # Create a Context on this device,\n",
    "        self.cfx = cuda.Device(0).make_context()\n",
    "        stream = cuda.Stream()\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "        # Deserialize the engine from file\n",
    "        with open(engine_file_path, \"rb\") as f:\n",
    "            engine = runtime.deserialize_cuda_engine(f.read())\n",
    "        context = engine.create_execution_context()\n",
    "\n",
    "        host_inputs = []\n",
    "        cuda_inputs = []\n",
    "        host_outputs = []\n",
    "        cuda_outputs = []\n",
    "        bindings = []\n",
    "\n",
    "        for binding in engine:\n",
    "            size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            cuda_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            # Append the device buffer to device bindings.\n",
    "            bindings.append(int(cuda_mem))\n",
    "            # Append to the appropriate list.\n",
    "            if engine.binding_is_input(binding):\n",
    "                host_inputs.append(host_mem)\n",
    "                cuda_inputs.append(cuda_mem)\n",
    "            else:\n",
    "                host_outputs.append(host_mem)\n",
    "                cuda_outputs.append(cuda_mem)\n",
    "\n",
    "        # Store\n",
    "        self.stream = stream\n",
    "        self.context = context\n",
    "        self.engine = engine\n",
    "        self.host_inputs = host_inputs\n",
    "        self.cuda_inputs = cuda_inputs\n",
    "        self.host_outputs = host_outputs\n",
    "        self.cuda_outputs = cuda_outputs\n",
    "        self.bindings = bindings\n",
    "\n",
    "    def infer(self, input_image_path):\n",
    "        # threading.Thread.__init__(self)\n",
    "        # Make self the active context, pushing it on top of the context stack.\n",
    "        self.cfx.push()\n",
    "        # Restore\n",
    "        stream = self.stream\n",
    "        context = self.context\n",
    "        engine = self.engine\n",
    "        host_inputs = self.host_inputs\n",
    "        cuda_inputs = self.cuda_inputs\n",
    "        host_outputs = self.host_outputs\n",
    "        cuda_outputs = self.cuda_outputs\n",
    "        bindings = self.bindings\n",
    "        # Do image preprocess\n",
    "        input_image, image_raw, origin_h, origin_w = self.preprocess_image(\n",
    "            input_image_path\n",
    "        )\n",
    "        # Copy input image to host buffer\n",
    "        np.copyto(host_inputs[0], input_image.ravel())\n",
    "        # Transfer input data  to the GPU.\n",
    "        cuda.memcpy_htod_async(cuda_inputs[0], host_inputs[0], stream)\n",
    "        # Run inference.\n",
    "        context.execute_async(bindings=bindings, stream_handle=stream.handle)\n",
    "        # Transfer predictions back from the GPU.\n",
    "        cuda.memcpy_dtoh_async(host_outputs[0], cuda_outputs[0], stream)\n",
    "        # Synchronize the stream\n",
    "        stream.synchronize()\n",
    "        # Remove any context from the top of the context stack, deactivating it.\n",
    "        self.cfx.pop()\n",
    "        # Here we use the first row of output in that batch_size = 1\n",
    "        output = host_outputs[0]\n",
    "        # Do postprocess\n",
    "        result_boxes, result_scores, result_classid = self.post_process(\n",
    "            output, origin_h, origin_w\n",
    "        )\n",
    "        # Draw rectangles and labels on the original image\n",
    "        for i in range(len(result_boxes)):\n",
    "            box = result_boxes[i]\n",
    "            plot_one_box(\n",
    "                box,\n",
    "                image_raw,\n",
    "                color=colors[int(result_classid[i])],\n",
    "                label=\"{}:{:.2f}\".format(\n",
    "                    categories[int(result_classid[i])], result_scores[i]\n",
    "                ),\n",
    "            )\n",
    "        parent, filename = os.path.split(input_image_path)\n",
    "        save_name = os.path.join(output_dir, filename)\n",
    "        # ã€€Save image\n",
    "        cv2.imwrite(save_name, image_raw)\n",
    "\n",
    "    def destroy(self):\n",
    "        # Remove any context from the top of the context stack, deactivating it.\n",
    "        self.cfx.pop()\n",
    "\n",
    "    def preprocess_image(self, input_image_path):\n",
    "        \"\"\"\n",
    "        description: Read an image from image path, convert it to RGB,\n",
    "                     resize and pad it to target size, normalize to [0,1],\n",
    "                     transform to NCHW format.\n",
    "        param:\n",
    "            input_image_path: str, image path\n",
    "        return:\n",
    "            image:  the processed image\n",
    "            image_raw: the original image\n",
    "            h: original height\n",
    "            w: original width\n",
    "        \"\"\"\n",
    "        image_raw = cv2.imread(input_image_path)\n",
    "        h, w, c = image_raw.shape\n",
    "        image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "        # Calculate widht and height and paddings\n",
    "        r_w = INPUT_W / w\n",
    "        r_h = INPUT_H / h\n",
    "        if r_h > r_w:\n",
    "            tw = INPUT_W\n",
    "            th = int(r_w * h)\n",
    "            tx1 = tx2 = 0\n",
    "            ty1 = int((INPUT_H - th) / 2)\n",
    "            ty2 = INPUT_H - th - ty1\n",
    "        else:\n",
    "            tw = int(r_h * w)\n",
    "            th = INPUT_H\n",
    "            tx1 = int((INPUT_W - tw) / 2)\n",
    "            tx2 = INPUT_W - tw - tx1\n",
    "            ty1 = ty2 = 0\n",
    "        # Resize the image with long side while maintaining ratio\n",
    "        image = cv2.resize(image, (tw, th))\n",
    "        # Pad the short side with (128,128,128)\n",
    "        image = cv2.copyMakeBorder(\n",
    "            image, ty1, ty2, tx1, tx2, cv2.BORDER_CONSTANT, (128, 128, 128)\n",
    "        )\n",
    "        image = image.astype(np.float32)\n",
    "        # Normalize to [0,1]\n",
    "        image /= 255.0\n",
    "        # HWC to CHW format:\n",
    "        image = np.transpose(image, [2, 0, 1])\n",
    "        # CHW to NCHW format\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        # Convert the image to row-major order, also known as \"C order\":\n",
    "        image = np.ascontiguousarray(image)\n",
    "        return image, image_raw, h, w\n",
    "\n",
    "    def xywh2xyxy(self, origin_h, origin_w, x):\n",
    "        \"\"\"\n",
    "        description:    Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "        param:\n",
    "            origin_h:   height of original image\n",
    "            origin_w:   width of original image\n",
    "            x:          A boxes tensor, each row is a box [center_x, center_y, w, h]\n",
    "        return:\n",
    "            y:          A boxes tensor, each row is a box [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "        r_w = INPUT_W / origin_w\n",
    "        r_h = INPUT_H / origin_h\n",
    "        if r_h > r_w:\n",
    "            y[:, 0] = x[:, 0] - x[:, 2] / 2\n",
    "            y[:, 2] = x[:, 0] + x[:, 2] / 2\n",
    "            y[:, 1] = x[:, 1] - x[:, 3] / 2 - (INPUT_H - r_w * origin_h) / 2\n",
    "            y[:, 3] = x[:, 1] + x[:, 3] / 2 - (INPUT_H - r_w * origin_h) / 2\n",
    "            y /= r_w\n",
    "        else:\n",
    "            y[:, 0] = x[:, 0] - x[:, 2] / 2 - (INPUT_W - r_h * origin_w) / 2\n",
    "            y[:, 2] = x[:, 0] + x[:, 2] / 2 - (INPUT_W - r_h * origin_w) / 2\n",
    "            y[:, 1] = x[:, 1] - x[:, 3] / 2\n",
    "            y[:, 3] = x[:, 1] + x[:, 3] / 2\n",
    "            y /= r_h\n",
    "\n",
    "        return y\n",
    "\n",
    "    def post_process(self, output, origin_h, origin_w):\n",
    "        \"\"\"\n",
    "        description: postprocess the prediction\n",
    "        param:\n",
    "            output:     A tensor likes [num_boxes,cx,cy,w,h,conf,cls_id, cx,cy,w,h,conf,cls_id, ...]\n",
    "            origin_h:   height of original image\n",
    "            origin_w:   width of original image\n",
    "        return:\n",
    "            result_boxes: finally boxes, a boxes tensor, each row is a box [x1, y1, x2, y2]\n",
    "            result_scores: finally scores, a tensor, each element is the score correspoing to box\n",
    "            result_classid: finally classid, a tensor, each element is the classid correspoing to box\n",
    "        \"\"\"\n",
    "        # Get the num of boxes detected\n",
    "        num = int(output[0])\n",
    "        # Reshape to a two dimentional ndarray\n",
    "        pred = np.reshape(output[1:], (-1, 6))[:num, :]\n",
    "        # to a torch Tensor\n",
    "        pred = torch.Tensor(pred).cuda()\n",
    "        # Get the boxes\n",
    "        boxes = pred[:, :4]\n",
    "        # Get the scores\n",
    "        scores = pred[:, 4]\n",
    "        # Get the classid\n",
    "        classid = pred[:, 5]\n",
    "        # Choose those boxes that score > CONF_THRESH\n",
    "        si = scores > CONF_THRESH\n",
    "        boxes = boxes[si, :]\n",
    "        scores = scores[si]\n",
    "        classid = classid[si]\n",
    "        # Trandform bbox from [center_x, center_y, w, h] to [x1, y1, x2, y2]\n",
    "        boxes = self.xywh2xyxy(origin_h, origin_w, boxes)\n",
    "        # Do nms\n",
    "        indices = torchvision.ops.nms(boxes, scores, iou_threshold=IOU_THRESHOLD).cpu()\n",
    "        result_boxes = boxes[indices, :].cpu()\n",
    "        result_scores = scores[indices].cpu()\n",
    "        result_classid = classid[indices].cpu()\n",
    "        return result_boxes, result_scores, result_classid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, func, args):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "\n",
    "    def run(self):\n",
    "        self.func(*self.args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load custom plugins\n",
    "    PLUGIN_LIBRARY = \"build/libmyplugins.so\"\n",
    "    ctypes.CDLL(PLUGIN_LIBRARY)\n",
    "    engine_file_path = \"build/yolov5t.engine\"\n",
    "    input_dir = \"inference/minival\"\n",
    "    output_dir = \"inference/output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # load coco labels\n",
    "\n",
    "    # categories = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    #               \"traffic light\",\n",
    "    #               \"fire hydrant\", \"stop sign\", \"parki ng meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "    #               \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
    "    #               \"frisbee\",\n",
    "    #               \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
    "    #               \"surfboard\",\n",
    "    #               \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    #               \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "    #               \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "    #               \"cell phone\",\n",
    "    #               \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    #               \"teddy bear\",\n",
    "    #               \"hair drier\", \"toothbrush\"]\n",
    "    categories = [\n",
    "        'vehicle',\n",
    "        'bicycle',\n",
    "        'person',\n",
    "        'sign',\n",
    "        'light',\n",
    "    ]\n",
    "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in categories]\n",
    "    # a  YoLov5TRT instance\n",
    "    yolov5_wrapper = YoLov5TRT(engine_file_path)\n",
    "\n",
    "    # from https://github.com/ultralytics/yolov5/tree/master/inference/images\n",
    "    input_image_paths = [os.path.join(input_dir, _) for _ in os.listdir(input_dir)]\n",
    "    start = time.time()\n",
    "    for input_image_path in tqdm(input_image_paths):\n",
    "        # create a new thread to do inference\n",
    "        # thread1 = myThread(yolov5_wrapper.infer, [input_image_path])\n",
    "        # thread1.start()\n",
    "        # thread1.join()\n",
    "\n",
    "        yolov5_wrapper.infer(input_image_path)\n",
    "\n",
    "    print(time.time() - start)\n",
    "    # destroy the instance\n",
    "    yolov5_wrapper.destroy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}